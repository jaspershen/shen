[{"authors":["admin"],"categories":null,"content":"I am now a postdoctoral research fellow at Stanford University. I am broadly interested in Metabolomics, Multi-omics, Biostatistics, Systems Biology, and Bioinformatics, and their application in healthcare.\nMy research focuses on the development of bioinformatic algorithms and tools for large-scale metabolomics as well as its application in discovering new biomarkers related to human diseases. I have developed several algorithms and tools in the past five years. The first algorithm was MetNormalizer, which is a machine learning-based algorithm for data normalization and integration of large-scale metabolomics. Then I developed a web-based tool, MetFlow, aiming to provide a comprehensive pipeline for data cleaning and statistical analysis of large-scale metabolomics. The most important algorithm I have developed is called MetDNA, which is a novel algorithm for metabolite identification and dysregulated pathway analysis in untargeted metabolomics.\n","date":1562198400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1562198400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am now a postdoctoral research fellow at Stanford University. I am broadly interested in Metabolomics, Multi-omics, Biostatistics, Systems Biology, and Bioinformatics, and their application in healthcare.\nMy research focuses on the development of bioinformatic algorithms and tools for large-scale metabolomics as well as its application in discovering new biomarkers related to human diseases. I have developed several algorithms and tools in the past five years. The first algorithm was MetNormalizer, which is a machine learning-based algorithm for data normalization and integration of large-scale metabolomics.","tags":null,"title":"Xiaotao Shen","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":["R"],"content":"\r线性回归\r使用直线拟合的方法得到拟合直线称之为线性回归(linear regression).如果只有一个自变量,则称之为简单线性回归(simple),有多个自变量,则称之为多元线性回归(multiple regression).这两个模型都必须满足变量是连续性变量,如果变量为分类变量,也可以进行线性回归,称之为逻辑回归(logistic regression).\n简单线性回归\r我们举个例子,年龄和身高是否有线性关系,是否可以使用年龄来预测身高呢?\nage \u0026lt;- 18:29\rheight \u0026lt;- c(76.1,77,78.1,78.2,78.8,79.7,79.9,81.1,81.2,81.8,82.8,83.5)\rdata \u0026lt;- data.frame(age, height, stringsAsFactors = FALSE)\rlibrary(ggplot2)\r(\rplot \u0026lt;- ggplot(data, aes(age, height)) +\rgeom_point() +\rtheme_bw()\r)\r我们画出散点图之后,可以看出age和height呈比较明显的线性关系.\n那我们怎么用一条直线去拟合(fit)数据呢?我们使用的就是最小平方差法(least square).\n那么最简单的拟合办法是什么呢?也就是直接使用所有人的height的平均值去拟合.\nplot +\rgeom_hline(yintercept = mean(data$height))\r那么怎么评价一条直线对数据拟合的好坏呢?这时候就需要引入另外一个概念,残差(residuls).残差也就是真实值和拟合值之间差值:\n\\[residual = y_i - y_i^{\u0026#39;}\\]\n其中\\(y_i\\)是真实值,而\\(y_i{\u0026#39;}\\)是模型预测值.\nplot +\rgeom_hline(yintercept = mean(data$height)) +\rgeom_segment(aes(x = data$age[1], y = data$height[1], xend = data$age[1], yend = mean(data$height)), colour = \u0026quot;red\u0026quot;, arrow = arrow()) \r从定义可以看到,残差有正有负.为了表示一条拟合直线对数据拟合的程度的好坏,我们将残差平方然后加和,从而得到残差平方和(sum of squared residuals),该值就可以用来表示一条直线对数据的拟合的好坏.拟合的越好,该值越小,拟合的越差,该值越大.\n\r其实使用残差平方和作为损失函数得到的最后的方程或者说模型并不一定是最优解,但是好处是方便后面求解(求导)并且解是唯一的.另外还有使用残差绝对值之和作为损失函数进行求解.以后再慢慢介绍.\n\r\\[sum \\quad of \\quad squared \\quad residuals = \\sum_{i=1}^n(y_i-y_i^{\u0026#39;})^2\\]\n\r注意,该值是有量纲的,只能比较同一组数据不同拟合直线的好坏,而不能比较不同数据之间的拟合的好坏.后面会介绍\\(R^2\\).\n\r那么怎样找到一个最好的拟合直线呢?我们可以将这条线(平均值)进行旋转,每条直线都会有截距(intersect)和斜率(slope)两个参数,对于每一条直线,我们都会计算其残差平方和.\n最后,我们会得到一系列的截距和斜率组合以及他们所对应的残差平方和.我们可以简单的理解,直接选取残差平方和最小的参数组合即可(实际情况更为复杂一些).从而得到最佳的参数组合.这也是为什么这种方法称之为’least square’的原因.\n那么如何估量一个拟合直线的好坏呢?这时候就需要引入\\(R^2\\)的概念.\n\r\\(R^2\\)\r首先,我们计算使用身高平均值拟合直线的残差平方和,称之为\\(SS(mean)\\)\n\\[SS(mean) = (data - mean)^2\\]\n同时,我们知道方差(Var(mean))等于残差平方和除以样本个数:\n\\[Var(mean) = \\frac{SS(mean)}{n}\\]\n然后,按照同样的方法,我们计算拟合直线的残差平方和,称之为\\(SS(fit)\\).同时,我们也可以计算出来拟合直线的方差(Var(fit)).\n可以看到,拟合直线得到的残差平方和要比平均值得到的要小,也就是说,将年龄因素考虑进去之后,身高的方差有一部分能够被年龄所解释.\n\\(R^2\\)告诉我们年龄可以解释多少比例的身高的方差.\n\\[R^2 = \\frac{Var(mean) - Var(fit)}{Var(mean)} = \\frac{SS(mean) - SS(fit)}{SS(mean)} = 1 - \\frac{ SS(fit)}{SS(mean)}\\]\n从公式可以看出,如果一条直线正好穿过所有的数据点,那么\\(SS(fit)=0\\),这时候\\(R^2=1\\).而如果身高和年龄完全没有关系,则\\(R^2 = 0\\).\n\r需要注意的是,\\(R^2\\)的大小并不具有统计学意义,我们可以想象,如果只有两个点,那么必定有一条直线通过他们,因此\\(SS(fit)\\)为0,但是很明显这样的fit是没有意义的,得到的\\(R^2\\)也并不能说明两个变量之间真是的关系.这说明了样本的数量也同样重要,因此,我们需要对\\(R^2\\)进行统计学假设检验,得到其统计学显著性,也就是p value.\n\r\r\\(R^2\\)的假设检验\r首先,我们需要明白\\(R^2\\)的含义,我们上面已经讨论过了,\\(R^2\\)指的是变量x能够解释变量y的方差的比例.比如上面的例子,\\(R^2\\)为0.99,说明年龄可以解释99%的身高的方差.\n这时候,需要引入一个统计量,F:\n\\[F = \\frac{(Var(mean) - Var(fit))/(p_{fit}-p_{mean})}{Var(fit)/(n-p_{fit})} = \\frac{(SS(mean) - SS(fit))/(p_{fit}-p_{mean})}{SS(fit)/(n-p_{fit})} = \\frac{\\frac{(SS(mean) - SS(fit))}{p_{fit}-p_{mean}}} {\\frac{SS(fit)}{n-p_{fit}}}\\]\n也就是说,F等于变量x能够解释变量y的方差除以变量x不能够解释变量y的方差.\n而\\((n-p_{fit})/(p_{fit}-p_{mean})\\)则称之为自由度(degrees of freedoms).自由度的定义以后再详细解释.\n其中\\(p_{fit}\\)代表回归方程中参数的数目,比如一元线性方程中,只有两个参数(intercept和slop),所以\\(p_{fit}=2\\).\\(p_{mean}\\)代表平均值直线的参数个数,在一元线性方程中,为1.n是数据集的样本个数.\n因此,\\(p_{fit}-p_{mean}\\)代表的含义是该拟合模型与平均值模型相比,多出来的参数的个数.在简单线性回归中,多出来的就是slope.对于多元线性回归,比如,两个自变量预测一个因变量,则这时候\\(p_{fit}=3\\).\n我们再来看分母,为什么需要使用\\(SS(fit)\\)除以\\(n-p_{fit}\\)呢?因为我们知道模型越复杂(参数越多),你就需要更多的点去拟合.比如,对于两个点的一元一次方程,只需要两个点即可(两点确定一条直线).而对于二元一次方程,则需要三个点去拟合.\n这时候我们再来看分子分母分别代表什么意义.\n\r分子:拟合模型多出来的参数能够解释的方差(变异).\n\r分母:拟合模型多出来的参数不能够解释的方差(变异).\n\r\r这样我们拿到F值以后,怎么计算p value呢?其实就是使用permutation test的方法.\n随机产生与数据集样本数相同数目的数据集.\n\r对其进行拟合并计算每一组随机数据的F值.\n\r产生大量的随机F值,并得到随机F值的分布.\n\r拿到随机F值的分布,p value就等于大于真实F值的数目除以随机F值的总数目.\n\r\r当然,所有随机F值的分布其实是符合F分布的,拿到了F分布,就可以计算p value.F分布以后再详细介绍.\n\r矫正\\(R^2\\)(\\(adjusted R^2\\))\r什么是矫正\\(R^2\\)呢?我们从\\(R^2\\)定义可以看到,如果我们增加了拟合模型的变量个数,因为\\(SS(mean)\\)是固定的,因此\\(SS(fit)\\)都会减小,所以导致\\(R^2\\)变大,从而造成过拟合(over fitting).因此,我们需要对模型选入的变量个数做一个惩罚,这就是\\(adjusted \\quad R^2\\).以后再详细解释.\n\r系数的假设检验\r同样的,我们可以看到,上面是对\\(R^2\\)做假设检验,当然我们也需要对每个变量的系数(包括intersect)进行假设验证,在多元回归中尤其重要,我们会在以后进行详细解释.\n\rR语言进行线性回归\rR语言中的lm()函数可以用来进行线性回归.使用方法如下:\nlm_fit \u0026lt;- lm(height ~ age, data)\rsummary(lm_fit)\r## ## Call:\r## lm(formula = height ~ age, data = data)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.27238 -0.24248 -0.02762 0.16014 0.47238 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 64.9283 0.5084 127.71 \u0026lt; 2e-16 ***\r## age 0.6350 0.0214 29.66 4.43e-11 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.256 on 10 degrees of freedom\r## Multiple R-squared: 0.9888, Adjusted R-squared: 0.9876 ## F-statistic: 880 on 1 and 10 DF, p-value: 4.428e-11\rsummary中提供了该线性回归的大部分内容.其中Multiple R-squared其实就是R-squared\\(R^2\\),而Adjusted R-Squred主要是对多元线性回归时对多个变量进行矫正,因为当变量数目增加的时候,\\(R^2\\)总会变小,因此需要对其进行矫正.对于简单线性回归来说.直接使用Multiple R-squared即可,而对于多元线性回归来说,则需要使用Adjusted R-Squred来衡量模型拟合的效果.\n\u0026lt;script src=\u0026quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;!-- MathJax configuration --\u0026gt;\r\u0026lt;script type=\u0026quot;text/x-mathjax-config\u0026quot;\u0026gt;\rMathJax.Hub.Config({\rtex2jax: {\rinlineMath: [ [\u0026#39;$\u0026#39;,\u0026#39;$\u0026#39;], [\u0026quot;\\\\(\u0026quot;,\u0026quot;\\\\)\u0026quot;] ],\rdisplayMath: [ [\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026quot;\\\\[\u0026quot;,\u0026quot;\\\\]\u0026quot;] ],\rprocessEscapes: true,\rprocessEnvironments: true\r},\r// Center justify equations in code and markdown cells. Elsewhere\r// we use CSS to left justify single line equations in code cells.\rdisplayAlign: \u0026#39;center\u0026#39;,\r\u0026quot;HTML-CSS\u0026quot;: {\rstyles: {\u0026#39;.MathJax_Display\u0026#39;: {\u0026quot;margin\u0026quot;: 0}},\rlinebreaks: { automatic: true }\r}\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;!-- End of mathjax configuration --\u0026gt;\r\r\r","date":1566777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566777600,"objectID":"a4aa8945bc00817a54934e6ea22f0b0a","permalink":"/post/2019-08-26-/","publishdate":"2019-08-26T00:00:00Z","relpermalink":"/post/2019-08-26-/","section":"post","summary":"线性回归 使用直线拟合的方法得到拟合直线称之为线性回归(linear regression).如果只有一个自变量,则称之为简单线性回归(simp","tags":["Blog"],"title":"生物统计学习:简单线性回归模型","type":"post"},{"authors":["Xiaotao Shen"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1562198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562198400,"objectID":"4008e7b082490319c6231570ec8b2583","permalink":"/post/blogdown-instruction/","publishdate":"2019-07-04T00:00:00Z","relpermalink":"/post/blogdown-instruction/","section":"post","summary":"使用blogdown包搭建个人blog","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":["Xiaotao Shen","Ruohong Wang","Others","Zheng-Jiang Zhu"],"categories":null,"content":"","date":1554336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554336000,"objectID":"63d45f7c777552440708a1b1b031e34c","permalink":"/publication/journal-article/metdna/","publishdate":"2019-04-04T00:00:00Z","relpermalink":"/publication/journal-article/metdna/","section":"publication","summary":"Here, we develop a metabolic reaction network (MRN)-based recursive algorithm (MetDNA) that expands metabolite annotations without the need for a comprehensive standard spectral library.","tags":["Source Themes"],"title":"Metabolic Reaction Network-based Recursive Metabolite Annotation for Untargeted Metabolomics","type":"publication"},{"authors":["Xiaotao Shen"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Zhuozhong Wang","Others","Xiaotao Shen"],"categories":null,"content":"","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"a70b8f565d8fadd32a7e9ba6fb8ad482","permalink":"/publication/journal-article/zhuozhong_ac/","publishdate":"2018-12-23T00:00:00Z","relpermalink":"/publication/journal-article/zhuozhong_ac/","section":"publication","summary":"The metabolic profiling of biofluids using untargeted metabolomics provides a promising choice to discover metabolite biomarkers for clinical cancer diagnosis. However, metabolite biomarkers discovered in biofluids may not necessarily reflect the pathological status of tumor tissue, which makes these biomarkers difficult to reproduce. In this study, we developed a new analysis strategy by integrating the univariate and multivariate correlation analysis approach to discover tumor tissue derived (TTD) metabolites in plasma samples. Specifically, untargeted metabolomics was first used to profile a set of paired tissue and plasma samples from 34 colorectal cancer (CRC) patients. Next, univariate correlation analysis was used to select correlative metabolite pairs between tissue and plasma, and a random forest regression model was utilized to define 243 TTD metabolites in plasma samples. The TTD metabolites in CRC plasma were demonstrated to accurately reflect the pathological status of tumor tissue and have great potential for metabolite biomarker discovery. Accordingly, we conducted a clinical study using a set of 146 plasma samples from CRC patients and gender-matched polyp controls to discover metabolite biomarkers from TTD metabolites. As a result, eight metabolites were selected as potential biomarkers for CRC diagnosis with high sensitivity and specificity. For CRC patients after surgery, the survival risk score defined by metabolite biomarkers also performed well in predicting overall survival time (p = 0.022) and progression-free survival time (p = 0.002). In conclusion, we developed a new analysis strategy which effectively discovers tumor tissue related metabolite biomarkers in plasma for cancer diagnosis and prognosis.","tags":["Metabolomics","Statistical analysis","Biomarker discovery"],"title":"Development of a Correlative Strategy To Discover Colorectal Tumor Tissue Derived Metabolite Biomarkers in Plasma Using Untargeted Metabolomics","type":"publication"},{"authors":["Xiaotao Shen","Zheng-Jiang Zhu"],"categories":null,"content":" Availability and implementation The MetFlow is freely available on http://metflow.zhulab.cn/.\nSupplementary information Supplementary data are available at Bioinformatics online.\n","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"30abc825074b5384f41953e1e2657efa","permalink":"/publication/journal-article/metflow/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/publication/journal-article/metflow/","section":"publication","summary":"Mass spectrometry-based metabolomics aims to profile the metabolic changes in biological systems and identify differential metabolites related to physiological phenotypes and aberrant activities.","tags":["Source Themes"],"title":"MetFlow an interactive and integrated workflow for metabolomics data cleaning and differential metabolite discovery","type":"publication"},{"authors":["Xiaotao Shen","Zheng-Jiang Zhu"],"categories":null,"content":" Availability and implementation The MetFlow is freely available on http://metflow.zhulab.cn/.\nSupplementary information Supplementary data are available at Bioinformatics online.\n","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"7cce40f13d0d2f49b7a67722e92d3bbe","permalink":"/publication/journal-article/metnormalizer/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/publication/journal-article/metnormalizer/","section":"publication","summary":"Mass spectrometry-based metabolomics aims to profile the metabolic changes in biological systems and identify differential metabolites related to physiological phenotypes and aberrant activities.","tags":["Source Themes"],"title":"MetFlow an interactive and integrated workflow for metabolomics data cleaning and differential metabolite discovery","type":"publication"},{"authors":["Huixun Jia","Xiaotao Shen","Others","Zheng-Jiang Zhu"],"categories":null,"content":"","date":1537660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537660800,"objectID":"c8b9c6c1e97c691488c4aa2cbf3c8cf3","permalink":"/publication/journal-article/huixun/","publishdate":"2018-09-23T00:00:00Z","relpermalink":"/publication/journal-article/huixun/","section":"publication","summary":"A panel of metabolites has been identified to facilitate the prediction of tumor response to NCRT in LARC, which is promising for the generation of personalized treatment strategies for LARC patients.","tags":["Metabolomics","Biomarker discovery"],"title":"Predicting the pathological response to neoadjuvant chemoradiation using untargeted metabolomics in locally advanced rectal cancer","type":"publication"},{"authors":["Zhiwei Zhou","Others","Xiaotao Shen","Zheng-Jiang Zhu"],"categories":null,"content":"","date":1532304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532304000,"objectID":"66ddc18e59fafe4fdab325a25fc321a7","permalink":"/publication/journal-article/lipidccs/","publishdate":"2018-07-23T00:00:00Z","relpermalink":"/publication/journal-article/lipidccs/","section":"publication","summary":"The use of collision cross-section (CCS) values derived from ion mobility–mass spectrometry (IM–MS) has been proven to facilitate lipid identifications. Its utility is restricted by the limited availability of CCS values. Recently, the machine-learning algorithm-based prediction (e.g., MetCCS) is reported to generate CCS values in a large-scale. However, the prediction precision is not sufficient to differentiate lipids due to their high structural similarities and subtle differences on CCS values. To address this challenge, we developed a new approach, namely, LipidCCS, to precisely predict lipid CCS values.","tags":["Lipidomics","Data processing","Software"],"title":"LipidCCS Prediction of Collision Cross-Section Values for Lipids with High Precision To Support Ion Mobility–Mass Spectrometry-Based Lipidomics","type":"publication"},{"authors":["Zhiwei Zhou","Xiaotao Shen","Others","Zheng-Jiang Zhu"],"categories":null,"content":"","date":1532304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532304000,"objectID":"aca20ec32852cec914c973d8c86adfc0","permalink":"/publication/journal-article/lipidimms/","publishdate":"2018-07-23T00:00:00Z","relpermalink":"/publication/journal-article/lipidimms/","section":"publication","summary":"Ion mobility—mass spectrometry (IM-MS) has showed great application potential for lipidomics. However, IM-MS based lipidomics is significantly restricted by the available software for lipid structural identification. Here, we developed a software tool, namely, LipidIMMS Analyzer, to support the accurate identification of lipids in IM-MS. For the first time, the software incorporates a large-scale database covering over 260 000 lipids and four-dimensional structural information for each lipid [i.e. m/z, retention time (RT), collision cross-section (CCS) and MS/MS spectra]. Therefore, multi-dimensional information can be readily integrated to support lipid identifications, and significantly improve the coverage and confidence of identification. Currently, the software supports different IM-MS instruments and data acquisition approaches.","tags":["Lipidomics","Data processing","Software"],"title":"LipidIMMS Analyzer integrating multi-dimensional information to support lipid identification in ion mobility—mass spectrometry based lipidomics","type":"publication"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1529845200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529845200,"objectID":"b95c40c62f937e204d0a4a497ac69376","permalink":"/talk/2018seattle/","publishdate":"2018-06-26T00:00:00Z","relpermalink":"/talk/2018seattle/","section":"talk","summary":"Metabolic Reaction Network-based Recursive Metabolite Identification for Untargeted Metabolomics","tags":[],"title":"Metabolic Reaction Network-based Recursive Metabolite Identification for Untargeted Metabolomics","type":"talk"},{"authors":["Zhiwei Zhou","Xiaotao Shen","Others","Zheng-Jiang Zhu"],"categories":null,"content":"","date":1477180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477180800,"objectID":"5266545b46548c81b97dbc477772e47f","permalink":"/publication/journal-article/metccs/","publishdate":"2016-10-23T00:00:00Z","relpermalink":"/publication/journal-article/metccs/","section":"publication","summary":"The use of collision cross-section (CCS) values derived from ion mobility–mass spectrometry (IM–MS) has been proven to facilitate lipid identifications. Its utility is restricted by the limited availability of CCS values. Recently, the machine-learning algorithm-based prediction (e.g., MetCCS) is reported to generate CCS values in a large-scale. However, the prediction precision is not sufficient to differentiate lipids due to their high structural similarities and subtle differences on CCS values. To address this challenge, we developed a new approach, namely, LipidCCS, to precisely predict lipid CCS values.","tags":["Metabolomics","Data processing","Software"],"title":"Large-Scale Prediction of Collision Cross-Section Values for Metabolites in Ion Mobility-Mass Spectrometry","type":"publication"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1469192400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469192400,"objectID":"d77bf136c5f2450d9150636396cbf599","permalink":"/talk/2016texas/","publishdate":"2016-07-22T00:00:00Z","relpermalink":"/talk/2016texas/","section":"talk","summary":"Normalization and Integration of Large-Scale Mass Spectrometry-based Metabolomics Data Using Support Vector Regression","tags":[],"title":"Normalization and Integration of Large-Scale Mass Spectrometry-based Metabolomics Data Using Support Vector Regression","type":"talk"},{"authors":null,"categories":null,"content":"MetDNA characterizes initial seed metabolites using a small tandem spectral library, and utilize their experimental MS2 spectra as surrogate spectra to annotate their reaction-paired neighbor metabolites which are subsequently served as the basis for recursive analysis.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"18513779dc9fd15c06d9e07f9e441e13","permalink":"/project/metdna-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/metdna-project/","section":"project","summary":"Metabololite identification and dysregulated network analysis.","tags":["Metabolomics"],"title":"MetDNA","type":"project"},{"authors":null,"categories":null,"content":"We developed an interactive web server, namely, MetFlow, to provide an integrated and comprehensive workflow for metabolomics data cleaning and differential metabolite discovery.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"42b72e49ed0a0bb4debb9632d105a4e5","permalink":"/project/metflow-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/metflow-project/","section":"project","summary":"Web Server for Metabolomics Data Cleaning and Differential Metabolite Discovery","tags":["Metabolomics"],"title":"MetFlow","type":"project"},{"authors":["Xiaotao Shen"],"categories":[],"content":" Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n Setup Academic Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Color Themes Academic comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":" 我的2016年 现在想想，其实整个2016年真的是没什么收获，科研上没什么进展，技能上也没有什么提升，原本准备要学习的python，也在1-2个月之后，彻底的放弃了。现在可能只记得一点点皮毛了。那么2016年的问题到底在哪里呢？总结来说，可以分为下面几部分。\n效率低下 不得不承认，我的学习效率真的是出奇的低下，从我上高中以来其实都是这样了，做什么事情都不能够专心致志。用高中同学利弟的话来说，是“玩的时候没有好好玩，学习的时候没有好好学习”，用最近学到的一句话是“用战术上的勤奋来掩饰战略上的懒惰”。具体表现就是精神不集中，开小车。\n懒惰 现在真的是越来越懒惰了，很多事情都没有以前有激情了，可能是我老了？\n没有明确的目标 上半年其实还是挺有目标的，但是到了下半年，目标突然就失焦了。定下一个目标，努力去实现，或者定下一个计划，努力去完成。\n上面就是我2016年的一个简短的总结了，当然不是只有这些问题，还有很多其他的问题，但是可能都是一些细节的问题，就不过多去写了。\n我的2017年计划 文章 很幸运的是自己还是发了两篇文章，但是都是很低分的，虽然够我毕业，但是其实自己知道并不足够，因此2017年的主要任务就是好好看文献，做实验，寻找思路，争取能够再发一篇高质量的你文章。\n统计 毕竟我的课题和统计关系很大，自己的统计数据功底很差，希望可以在看文献以及资料的同时，好好学习一下统计，机器学习等知识，再课题中能够真正运用到。\nR R是一个好东西，需要再深入的学习，主要是结合着统计去学习，并及时做好总计，以有道笔记和博客作为平台去记录自己的学习，即作为笔记也作为总结。\npython 这个放到最后，是因为可能很难有很多时间去学习了，希望前面几条比较顺利，才会有时间去做。\n健身 经过别人的提醒，还有这一项，确实自己看起来太瘦了，尤其是上半身，连衬衫和西装都撑不起来，等天气开始暖和了，就开始跑步，然后等中期答辩结束之后，考虑办一张健身卡，去健身！\n最后的最后，提醒自己，最后一年，加油！\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a63003d36cb5bce1cee7244b2ca1682","permalink":"/post/2017-02-12-firstblog2017/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2017-02-12-firstblog2017/","section":"post","summary":"我的2016年 现在想想，其实整个2016年真的是没什么收获，科研上没什么进展，技能上也没有什么提升，原本准备要学习的python，也在1-2","tags":["life"],"title":"2017年的第一篇博客","type":"post"},{"authors":null,"categories":null,"content":" Introduction MetCleaning provides an integrated and automatic pipeline for data cleaning and statistical analysis of large scale mass spectrometry (MS) based-metabolomic data. It includes missing value (MV) filtering and imputation, zero value filtering, data normalization, data integration, data quality assessment, univariate statistical analysis, multivariate statistical analysis such as PCA and PLS-DA, potential marker selection and show. This document describes how to use the integrated functions, MetClean and MetStat in MetCleaning utilizing demo data.\nInstallation and help MetCleaning is published in github (link). So you can install it via to github.\ncode 1: Installation of MetCleaning\n##pcaMethods and impute should be installed form bioconductor ##pcaMethos source(\u0026quot;http://bioconductor.org/biocLite.R\u0026quot;) biocLite(\u0026quot;pcaMethods\u0026quot;) ##impute source(\u0026quot;http://bioconductor.org/biocLite.R\u0026quot;) biocLite(\u0026quot;impute\u0026quot;) if(!require(devtools)) { install.packages(\u0026quot;devtools\u0026quot;) } library(devtools) install_github(\u0026quot;jaspershen/MetCleaning\u0026quot;) library(MetCleaning) help(package = \u0026quot;MetCleaning\u0026quot;)  Data cleaning Data cleaning is integrated as a function named as MetClean in MetCleaning. We use the demo data as the example. Copy the code below and paste in you R console.\ncode 2: Demo data of MetClean\n##demo data data(data, package = \u0026quot;MetCleaning\u0026quot;) data(sample.information, package = \u0026quot;MetCleaning\u0026quot;) ##demo work directory dir.create(\u0026quot;Demo for MetCleaning\u0026quot;) setwd(\u0026quot;Demo for MetCleaning\u0026quot;) ##write files write.csv(data, \u0026quot;data.csv\u0026quot;, row.names = FALSE) write.csv(sample.information , \u0026quot;sample.information.csv\u0026quot;, row.names = FALSE)  The demo data have been added in your work directory and organized in you work directory as Figure 2 shows. It contains two files, \u0026ldquo;data.csv\u0026rdquo; and \u0026ldquo;sample.information.csv\u0026rdquo;. 1. \u0026ldquo;data.csv\u0026rdquo; is the metabolomic dataset you want to process. Rows are features and columns are feature abundance of samples and information of features. The information of features must contain \u0026ldquo;name\u0026rdquo; (feature name), \u0026ldquo;mz\u0026rdquo; (mass to change ratio) and \u0026ldquo;rt\u0026rdquo; (retention time). Other information of features are optional, for example \u0026ldquo;isotopes\u0026rdquo; and \u0026ldquo;adducts\u0026rdquo;. The name of sample can contain \u0026ldquo;.\u0026rdquo;, but cannot contain \u0026ldquo;-\u0026rdquo; and space. And the start of sample name cannot be number. For example, \u0026ldquo;A210.a\u0026rdquo; and \u0026ldquo;A210a\u0026rdquo; are valid, and \u0026ldquo;210a\u0026rdquo; or \u0026ldquo;210-a\u0026rdquo; are invalid. 2. \u0026ldquo;sample.information.csv\u0026rdquo; is sample information for metabolomic dataset. Column 1 is \u0026ldquo;sample.name\u0026rdquo; which is the names of subject and QC samples. Please confirm that the sample names in \u0026ldquo;sample.information.csv\u0026rdquo; and \u0026ldquo;data.csv\u0026rdquo; are completely same. Column 2 is \u0026ldquo;injection.order\u0026rdquo; which is the injection order of QC and subject samples. Column 3 is \u0026ldquo;class\u0026rdquo;, which is used to distinguish \u0026ldquo;QC\u0026rdquo; and \u0026ldquo;Subject\u0026rdquo; samples. Column 4 is \u0026ldquo;batch\u0026rdquo; to provide acquisition batch information for samples. Column 5 is \u0026ldquo;group\u0026rdquo;, which is used to label the group of subject sample, for example, \u0026ldquo;control\u0026rdquo; and \u0026ldquo;case\u0026rdquo;. The \u0026ldquo;group\u0026rdquo; of QC samples is labeled as \u0026ldquo;QC\u0026rdquo;.\nThen you can run MetClean function to do data cleaning of data. All the arguments of MetClean can be found in the other functions in MetCleaning. You can use help(package = \u0026ldquo;MetCleaning\u0026rdquo;) to see the help page of MetCleaning.\ncode 3: Running of MetClean\n##demo data MetClean(polarity = \u0026quot;positive\u0026quot;)  Running results of MetClean 1.Missing or zero values filtering. In the missing or zero value filtering step, if there are samples which beyond the threshold you set, you should decide to filter them or not. We recommend to remove all of them as Figure 3 shows.\n2.Sample filtering. In the QC or subject sample filtering step (based on PCA), if there are samples which beyond the threshold you set, you should decide to filter them or not. We don\u0026rsquo;t recommend to remove them as Figure 4 shows, because they should be consired combined other information.\n3.Output files. Output files of MetClean are listed as Figure 5 shows. (1) \u0026ldquo;1MV overview\u0026rdquo;, \u0026ldquo;2MV filter\u0026rdquo;, \u0026ldquo;3Zero overview\u0026rdquo; and \u0026ldquo;4Zero filter\u0026rdquo; are missing and zero values filtering information. (2) \u0026ldquo;5QC outlier filter\u0026rdquo; and \u0026ldquo;6Subject outlier filter\u0026rdquo; are sample filtering based on PCA information. (3) \u0026ldquo;7Normalization result\u0026rdquo; is the data normalization information for each batch. (4) \u0026ldquo;8Batch effect\u0026rdquo; is the batch effect both in before and after data cleaning. (5) \u0026ldquo;9metabolite plot\u0026rdquo; is the scatter plot for each feature. (6) \u0026ldquo;10Data overview\u0026rdquo; is the overview of data. (7) \u0026ldquo;11RSD overview\u0026rdquo; is the RSD distribution for each batch both before and after data cleaning. (8) \u0026ldquo;data_after_pre.csv\u0026rdquo;, \u0026ldquo;qc.info.csv\u0026rdquo; and \u0026ldquo;subject.info\u0026rdquo; are the data and sample information after data cleaning. (9) \u0026ldquo;intermediate\u0026rdquo; is the intermediate data during processing.\nStatistical analysis Data statistical analysis is integrated as a function named as MetStat in MetCleaning. We use the demo data as the example. Please note that now MetStat can only process two class data. Copy the code below and paste in you R console.\ncode 4: Demo data of MetStat\ndata(\u0026quot;met.data.after.pre\u0026quot;, package = \u0026quot;MetCleaning\u0026quot;) data(new.group, package = \u0026quot;MetCleaning\u0026quot;) ##create a folder for MetStat demo dir.create(\u0026quot;Demo for MetStat\u0026quot;) setwd(\u0026quot;Demo for MetStat\u0026quot;) ## export the demo data as csv write.csv(new.group, \u0026quot;new.group.csv\u0026quot;, row.names = FALSE)  The demo data have been added in your work directory. \u0026ldquo;new.group.csv\u0026rdquo; is a sample.information which has been changed the group information you want to use for statistical analysis. For the sample which you don\u0026rsquo;t want to use them for statistical analysis, you can set they group information as NA like Figure 6 shows.\ncode 5: Running of MetStat\nMetStat(MetFlowData = met.data.after.pre, new.group = TRUE)  Running results of MetStat 1.Sample removing. Firstly, you need to confirm the samples which you want to remove form dataset as Figure 7 shows.\n2.Number of component selection in PLS-DA analysis. In PLS-DA analysis, you should manually select the best choice of the number of component. When the Console show \u0026ldquo;How many comps do you want to see?\u0026rdquo;, you can type 10 and enter \u0026ldquo;Enter\u0026rdquo; key. Then a MSE plot is showing, and the best number of component is the one has the smallest CV values. So type the number (in this example is 4) and enter \u0026ldquo;Enter\u0026rdquo; key.\n3.Output files. Output files of MetStat are listed as Figure 9 shows. (1) \u0026ldquo;12PCA analysis\u0026rdquo; is the PCA score plot. (2) \u0026ldquo;13PLS analysis\u0026rdquo; contains the PLS-DA results. (3) \u0026ldquo;14heatmap\u0026rdquo; is the heatmap. (4) \u0026ldquo;15marker selection\u0026rdquo; contains the information of markers, volcano plot and boxplots of markers. (5) \u0026ldquo;data_after_stat.csv\u0026rdquo;, \u0026ldquo;qc.info.csv\u0026rdquo; and \u0026ldquo;subject.info\u0026rdquo; are the data and sample information after statistical analysis. (6) \u0026ldquo;intermediate\u0026rdquo; is the intermediate data during processing.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"89f470a1ce5ae9218b78de70a7aabbd0","permalink":"/post/2016-11-25-metcleaning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2016-11-25-metcleaning/","section":"post","summary":"Introduction MetCleaning provides an integrated and automatic pipeline for data cleaning and statistical analysis of large scale mass spectrometry (MS) based-metabolomic data. It includes missing value (MV) filtering and imputation, zero value filtering, data normalization, data integration, data quality assessment, univariate statistical analysis, multivariate statistical analysis such as PCA and PLS-DA, potential marker selection and show. This document describes how to use the integrated functions, MetClean and MetStat in MetCleaning utilizing demo data.","tags":null,"title":"MetCleaning instruction","type":"post"},{"authors":null,"categories":["R software","Metabolomics"],"content":" Ⅰ数据准备 MetDNA需要准备的数据包括一级数据peak table(csv格式)，二级数据(mgf格式)和样品信息sample.info(csv格式)。点击下载正离子demo数据和负离子demo数据。 Table 1: demo数据信息    组别 个数 含义     QC 8 QC   W03 10 野生型3天   W30 10 野生型30天   E03 10 突变型E3天   E30 10 突变型E30天   P03 10 突变型P3天   P30 10 突变型P30天    1. 一级数据 一级数据可以是使用XCMS，MZmine，MS-DIAL或者其他软件处理之后的数据。第一列为代谢物峰的名字，\u0026rdquo;name\u0026rdquo;，第二列为\u0026rdquo;mz\u0026rdquo;，第三列为保留时间(RT)，且单位必须为秒，其他为样品的峰强度。 2. 二级数据 二级质谱原始数据可以是使用QC样品采集的DDA或者targeted MS/MS数据。对于DDA数据来说，也可以是分段采集的二级数据。质谱原始二级数据需要使用ProteoWizard软件转为mgf格式，转换时参数设置参考下图。二级数据最多不能超过十个。 Figure 2: ProteoWizard参数设置\n3. 样品信息 样品信息是样品的分组信息。第一列是样品名，\u0026rdquo;sample.name\u0026rdquo;，第二列是样品的分组信息，\u0026rdquo;group\u0026rdquo;。 Ⅱ 数据整理 如果是正离子数据，请建立一个新的文件夹，命名为\u0026rdquo;POS\u0026rdquo;，如果是负离子数据，请建立一个新的文件夹命名为\u0026rdquo;NEG\u0026rdquo;，然后将一级数据，二级数据和样品信息放置于此文件夹下。并将该文件夹设置为工作路径。现在*MetDNA*部署在小服务器上，因此可以将数据放在小服务器中(labdata)。例如\u0026rdquo;V:/workreport/申小涛/demo/fly/POS\u0026rdquo;。 设置工作路径。 setwd(\u0026quot;/mnt/data/samba/labdata/workreport/申小涛/demo/fly/POS\u0026quot;) library(MetDNA)  Ⅲ 数据处理 1. 只对正离子或者负离子处理 所有的步骤可以使用一个函数*MetDNA*全部完成。 运行函数*MetDNA*。 MetDNA(ms1.file = \u0026quot;data.csv\u0026quot;, polarity = \u0026quot;positive\u0026quot;, column = \u0026quot;hilic\u0026quot;, ce = \u0026quot;30\u0026quot;, prefer.adduct = \u0026quot;M+H\u0026quot;, use.default.md = TRUE, threads = 3, group = c(\u0026quot;W03\u0026quot;, \u0026quot;W30\u0026quot;), uni.test = \u0026quot;t\u0026quot;, correct = TRUE, p.cutoff = 0.01, species = \u0026quot;dme\u0026quot;)   参数含义如下：\n  #### ms1.file：一级数据的名字。 #### polarity：数据采集极性，\u0026rdquo;positive\u0026rdquo;或者\u0026rdquo;negative\u0026rdquo;。 #### column：使用的柱子类型，\u0026rdquo;hilic\u0026rdquo;或者\u0026rdquo;rp\u0026rdquo;。 #### ce：二级采集的碰撞能量，支持\u0026rdquo;10\u0026rdquo;，\u0026rdquo;15\u0026rdquo;，\u0026rdquo;20\u0026rdquo;，\u0026rdquo;25\u0026rdquo;，\u0026rdquo;30\u0026rdquo;，\u0026rdquo;35\u0026rdquo;，\u0026rdquo;35,15\u0026rdquo; (35±15)，\u0026rdquo;40\u0026rdquo;， \u0026ldquo;45\u0026rdquo;，\u0026rdquo;50\u0026rdquo;，\u0026rdquo;55\u0026rdquo;，\u0026rdquo;60\u0026rdquo;，\u0026rdquo;65\u0026rdquo;，\u0026rdquo;70\u0026rdquo;。 #### prefer.adduct：使用那些加合物形式的注释用于RT预测模型的建立，默认使用所有的注释，推荐正离子模式下使用\u0026rdquo;M+H\u0026rdquo;，负离子模式下使用\u0026rdquo;M-H\u0026rdquo;。 #### use.default.md：进行保留时间预测模型建立时，是否使用默认的分子描述符，如果设置为FALSE，则会根据你的数据自动选择分子描述符。 #### threads：使用线程数，默认为3，可以根据电脑本身配置进行修改。 #### group：要对哪些分组的样品进行分析，注意，计算fold change时，使用后面的样品除以前面的样品。 #### uni.test：单变量分析的方法，\u0026rdquo;t\u0026rdquo;，Student t test；\u0026rdquo;wilcox\u0026rdquo;，Wilcox test。 #### correct：是否需要对p值进行FDR校正。 #### p.cutoff：选择dysregulated peak时的p值cutoff。 #### species：所研究样品的物种来源，\u0026rdquo;dme\u0026rdquo;，果蝇；\u0026rdquo;hsa\u0026rdquo;，人类；\u0026rdquo;mmu\u0026rdquo;，小鼠；\u0026rdquo;rat\u0026rdquo;，大鼠，\u0026rdquo;bta\u0026rdquo;，牛；\u0026rdquo;gga\u0026rdquo;，Gallus gallus (鸡)；\u0026rdquo;dre\u0026rdquo;，Danio rerio (斑马鱼)；\u0026rdquo;cel\u0026rdquo;，Caenorharomyces elegans (线虫)；\u0026rdquo;sce\u0026rdquo;，Saccharomyces cerevisaiae (酵母)； \u0026ldquo;ath\u0026rdquo;，Arabidopsis thaliana (拟南芥)；\u0026rdquo;smm\u0026rdquo;，Schistosoma mansoni；\u0026rdquo;pfa\u0026rdquo;，Plasmodum falciparum 3D7；\u0026rdquo;tbr\u0026rdquo;，Trypanosoma brucei；\u0026rdquo;eco\u0026rdquo;， Escherichia coli K-12 MG1655(大肠杆菌)；\u0026rdquo;ppu\u0026rdquo;，Pseudomonas putida KT2440；\u0026rdquo;syf\u0026rdquo;，Synechococcus elongatus。  2. 对正负数据合并分析 正负离子分别处理之后，可以使用函数*metModule2*函数合并正负离子模式的鉴定结果，进行dysregulated network analysis。 运行函数MetModule2 metModule2(group = c(\u0026quot;W03\u0026quot;, \u0026quot;W30\u0026quot;), uni.test = \u0026quot;t\u0026quot;, column = \u0026quot;hilic\u0026quot;, correct = TRUE, p.cutoff = 0.01, threads = 3, species = \u0026quot;dme\u0026quot;)   参数含义如下：\n  #### group：要对哪些分组的样品进行分析，注意，计算fold change时，使用后面的样品除以前面的样品。 #### uni.test：单变量分析的方法，\u0026rdquo;t\u0026rdquo;，Student t test；\u0026rdquo;wilcox\u0026rdquo;，Wilcox test。 #### column：使用的柱子类型，\u0026rdquo;hilic\u0026rdquo;或者\u0026rdquo;rp\u0026rdquo;。 #### correct：是否需要对p值进行FDR校正。 #### p.cutoff：选择dysregulated peak时的p值cutoff。 #### threads：使用线程数，默认为3，可以根据电脑本身配置进行修改。 #### species：所研究样品的物种来源，\u0026rdquo;dme\u0026rdquo;，果蝇；\u0026rdquo;hsa\u0026rdquo;，人类；\u0026rdquo;mmu\u0026rdquo;，小鼠；\u0026rdquo;rat\u0026rdquo;，大鼠，\u0026rdquo;bta\u0026rdquo;，牛；\u0026rdquo;gga\u0026rdquo;，Gallus gallus (鸡)；\u0026rdquo;dre\u0026rdquo;，Danio rerio (斑马鱼)；\u0026rdquo;cel\u0026rdquo;，Caenorharomyces elegans (线虫)；\u0026rdquo;sce\u0026rdquo;，Saccharomyces cerevisaiae (酵母)； \u0026ldquo;ath\u0026rdquo;，Arabidopsis thaliana (拟南芥)；\u0026rdquo;smm\u0026rdquo;，Schistosoma mansoni；\u0026rdquo;pfa\u0026rdquo;，Plasmodum falciparum 3D7；\u0026rdquo;tbr\u0026rdquo;，Trypanosoma brucei；\u0026rdquo;eco\u0026rdquo;， Escherichia coli K-12 MG1655(大肠杆菌)；\u0026rdquo;ppu\u0026rdquo;，Pseudomonas putida KT2440；\u0026rdquo;syf\u0026rdquo;，Synechococcus elongatus。  运行函数*analysisReport*得到分析报告。 analysisReport(polarity = \u0026quot;both\u0026quot;)  Ⅳ 运行结果 1. 正离子或者负离子的运行结果 MetDNA函数运行结束之后，所有的运行结果都存放在设置的路径中，包含二级谱图匹配结果，MRN注释结果，dysregulated network分析结果以及分析报告。如图5所示。 (1) MetDNA.parameters.csv 记录此次运行所使用的参数。 (2) ms2_match_result_POS 二级谱图匹配结果。包括一个csv文件，ms2.match.annotation.result.csv和一个文件夹，MS2_match_spectra。ms2.macth.annotation.result.csv是二级谱图匹配之后的结果，与MetAnalyzer处理之后的结果相同；MS2_match_spectra文件夹中包含了所有二级匹配图(Figure 6)。 (3) MRN_annotation_result_POS 基于metabolic reacion network注释结果。包括两个csv文件，metABM.parameters.csv和MRN.annotation.result.csv。MRN.annotation.result.csv是使用MRN注释结果(Figure 8)。 其中：  #### annotation.from.ID：该peak的该注释来自于哪个metabolite(ID); #### annotation.from.peak：该peak的该注释来自于哪个peak; #### ID：注释代谢物结果的KEGG ID; #### compound.name：注释结果的名字； #### isotope：同位素信息； #### adduct：加合物信息； #### Formula：化学结构式； #### score：注释打分； #### peak.group：peak group； #### confidence：对注释的peak group打分。  (4) Dysregulated_network_analsysi_result_POS Dysregulated network分析的结果。其中包括一个pdf文件，volcano.plot.pdf(Figure: 9)，两个csv文件，metModule.parameters.csv和DNA.annotation.result.csv以及两个文件夹，module_information(Figure 10)和pathway_inforamtion (Figure 11)。 1) volcano.plot是选取差异代谢物峰的火山图。 2) DNA.annotation.result.csv是通过dysregulated network对注释结果进行筛选以及KEGG database注释之后的注释结果。 3) module_information文件夹中包含了dysregulated module的分析结果。其中module.result.csv是module的信息。module.overview.pdf是module结果的总览(Figure 10)。module.heatmap.pdf是对module进行定量分析之后的热图(Figure 11)。boxplot文件夹中包含了module在两组间的定量结果(Figure 12)。Module_MSE analysis文件夹中包含了对每个module进行功能注释(MSEA)的结果(Figure 13)。 3) pathway_information文件夹中包含了pathway的分析结果(Figure 14)。 其中：  #### boxplot：该文件夹中含有每个pathway的定量信息; #### dysregulated.network.overview：该图表示dysregulated network的pathway分析结果; #### dysregulated.netwrok.MSEA.csv：dysregulated network的MSEA分析结果; #### dysregulated.netwrok.MSEA.pdf：dysregulated network的MSEA分析结果; #### dysregulated.networks.for.cytoscape.txt：用于cytoscape作图的数据； #### dysregulated.networks.attribute.txt：用于cytoscape作图的节点属性数据； #### pathway.heatmap.pdf：dysregulated network的pathway定量的heatmap。  (5) Analysis_report_POS 对数据处理分析结果的总结。输出的结果存放在Analysis_report文件夹内。包括一份html格式的分析报告。 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f19ac5b339c8d2a4a00cc2e658bfe662","permalink":"/post/2017-09-09-metdna-blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2017-09-09-metdna-blog/","section":"post","summary":"Ⅰ数据准备 MetDNA需要准备的数据包括一级数据peak table(csv格式)，二级数据(mgf格式)和样品信息sample.info(c","tags":["Chinese","R","Metabolomics"],"title":"MetDNA instruction","type":"post"},{"authors":["Xiaotao Shen","Others","Zheng-Jiang Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"712744e1d0e94e0e40b39dbf3ed057b1","permalink":"/publication/journal-article/taozhang_metabolomics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/journal-article/taozhang_metabolomics/","section":"publication","summary":"Untargeted metabolomics studies for biomarker discovery often have hundreds to thousands of human samples. Data acquisition of large-scale samples has to be divided into several batches and may span from months to as long as several years. The signal drift of metabolites during data acquisition (intra- and inter-batch) is unavoidable and is a major confounding factor for large-scale metabolomics studies. We aim to develop a data normalization method to reduce unwanted variations and integrate multiple batches in large-scale metabolomics studies prior to statistical analyses. We developed a machine learning algorithm-based method, support vector regression (SVR), for large-scale metabolomics data normalization and integration. An R package named MetNormalizer was developed and provided for data processing using SVR normalization. After SVR normalization, the portion of metabolite ion peaks with relative standard deviations (RSDs) less than 30 % increased to more than 90 % of the total peaks, which is much better than other common normalization methods. The reduction of unwanted analytical variations helps to improve the performance of multivariate statistical analyses, both unsupervised and supervised, in terms of classification and prediction accuracy so that subtle metabolic changes in epidemiological studies can be detected. SVR normalization can effectively remove the unwanted intra- and inter-batch variations, and is much better than other common normalization methods.","tags":["Metabolomics","Data cleaning","Software"],"title":"Normalization and integration of large-scale metabolomics data using support vector regression","type":"publication"},{"authors":null,"categories":null,"content":" Github很好的将代码和社区联系在了一起，于是发生了很多有趣的事情，世界也因为他美好了一点点。Github作为现在最流行的代码仓库，已经得到很多大公司和项目的青睐，比如jQuery、Twitter等。为使项目更方便的被人理解，介绍页面少不了，甚至会需要完整的文档站，Github替你想到了这一点，他提供了Github Pages的服务，不仅可以方便的为项目建立介绍站点，也可以用来建立个人博客。\nGithub Pages有以下几个优点：\n 轻量级的博客系统，没有麻烦的配置 使用标记语言，比如Markdown 无需自己搭建服务器 根据Github的限制，对应的每个站有300MB空间 可以绑定自己的域名  当然他也有缺点：\n 使用Jekyll模板系统，相当于静态页发布，适合博客，文档介绍等。 动态程序的部分相当局限，比如没有评论，不过还好我们有解决方案。 基于Git，很多东西需要动手，不像Wordpress有强大的后台  大致介绍到此，作为个人博客来说，简洁清爽的表达自己的工作、心得，就已达目标，所以Github Pages是我认为此需求最完美的解决方案了。\n购买、绑定独立域名 虽说Godaddy曾支持过SOPA，并且首页放着极其不专业的大胸美女，但是作为域名服务商他做的还不赖，选择它最重要的原因是他支持支付宝，没有信用卡有时真的很难过。\n域名的购买不用多讲，注册、选域名、支付，有网购经验的都毫无压力，优惠码也遍地皆是。域名的配置需要提醒一下，因为伟大英明的GFW的存在，我们必须多做些事情。\n流传Godaddy的域名解析服务器被墙掉，导致域名无法访问，后来这个事情在BeiYuu也发生了，不得已需要把域名解析服务迁移到国内比较稳定的服务商处，这个迁移对于域名来说没有什么风险，最终的控制权还是在Godaddy那里，你随时都可以改回去。\n我们选择DNSPod的服务，他们的产品做得不错，易用、免费，收费版有更高端的功能，暂不需要。注册登录之后，按照DNSPod的说法，只需三步（我们插入一步）：\n 首先添加域名记录，可参考DNSPod的帮助文档：https://www.dnspod.cn/Support 在DNSPod自己的域名下添加一条A记录，地址就是Github Pages的服务IP地址：207.97.227.245 在域名注册商处修改DNS服务:去Godaddy修改Nameservers为这两个地址：f1g1ns1.dnspod.net、f1g1ns2.dnspod.net。如果你不明白在哪里修改，可以参考这里：Godaddy注册的域名如何使用DNSPod 等待域名解析生效  域名的配置部分完成，跪谢方校长。\n配置和使用Github Git是版本管理的未来，他的优点我不再赘述，相关资料很多。推荐这本Git中文教程。\n要使用Git，需要安装它的客户端，推荐在Linux下使用Git，会比较方便。Windows版的下载地址在这里：http://code.google.com/p/msysgit/downloads/list。其他系统的安装也可以参考官方的安装教程。\n下载安装客户端之后，各个系统的配置就类似了，我们使用windows作为例子，Linux和Mac与此类似。\n在Windows下，打开Git Bash，其他系统下面则打开终端（Terminal）： 1、检查SSH keys的设置 首先我们需要检查你电脑上现有的ssh key：\n$ cd ~/.ssh  如果显示“No such file or directory”，跳到第三步，否则继续。\n2、备份和移除原来的ssh key设置： 因为已经存在key文件，所以需要备份旧的数据并删除：\n$ ls config id_rsa id_rsa.pub known_hosts $ mkdir key_backup $ cp id_rsa* key_backup $ rm id_rsa*  3、生成新的SSH Key： 输入下面的代码，就可以生成新的key文件，我们只需要默认设置就好，所以当需要输入文件名的时候，回车就好。\n$ ssh-keygen -t rsa -C \u0026quot;邮件地址@youremail.com\u0026quot; Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):\u0026lt;回车就好\u0026gt;  然后系统会要你输入加密串（Passphrase）：\nEnter passphrase (empty for no passphrase):\u0026lt;输入加密串\u0026gt; Enter same passphrase again:\u0026lt;再次输入加密串\u0026gt;  最后看到这样的界面，就成功设置ssh key了： 4、添加SSH Key到GitHub： 在本机设置SSH Key之后，需要添加到GitHub上，以完成SSH链接的设置。\n用文本编辑工具打开id_rsa.pub文件，如果看不到这个文件，你需要设置显示隐藏文件。准确的复制这个文件的内容，才能保证设置的成功。\n在GitHub的主页上点击设置按钮： 选择SSH Keys项，把复制的内容粘贴进去，然后点击Add Key按钮即可： PS：如果需要配置多个GitHub账号，可以参看这个多个github帐号的SSH key切换，不过需要提醒一下的是，如果你只是通过这篇文章中所述配置了Host，那么你多个账号下面的提交用户会是一个人，所以需要通过命令git config --global --unset user.email删除用户账户设置，在每一个repo下面使用git config --local user.email '你的github邮箱@mail.com' 命令单独设置用户账户信息\n5、测试一下 可以输入下面的命令，看看设置是否成功，git@github.com的部分不要修改：\n$ ssh -T git@github.com  如果是下面的反应：\nThe authenticity of host 'github.com (207.97.227.239)' can't be established. RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. Are you sure you want to continue connecting (yes/no)?  不要紧张，输入yes就好，然后会看到：\nHi \u0026lt;em\u0026gt;username\u0026lt;/em\u0026gt;! You've successfully authenticated, but GitHub does not provide shell access.  6、设置你的账号信息 现在你已经可以通过SSH链接到GitHub了，还有一些个人信息需要完善的。\nGit会根据用户的名字和邮箱来记录提交。GitHub也是用这些信息来做权限的处理，输入下面的代码进行个人信息的设置，把名称和邮箱替换成你自己的，名字必须是你的真名，而不是GitHub的昵称。\n$ git config --global user.name \u0026quot;你的名字\u0026quot; $ git config --global user.email \u0026quot;your_email@youremail.com\u0026quot;  设置GitHub的token 2012-4-28补充：新版的接口已经不需要配置token了，所以下面这段可以跳过了\n有些工具没有通过SSH来链接GitHub。如果要使用这类工具，你需要找到然后设置你的API Token。\n在GitHub上，你可以点击*Account Setting \u0026gt; Account Admin*： 然后在你的命令行中，输入下面的命令，把token添加进去：\n$ git config --global user.name \u0026quot;你的名字\u0026quot; $ git config --global user.token 0123456789your123456789token  如果你改了GitHub的密码，需要重新设置token。\n成功了 好了，你已经可以成功连接GitHub了。\n使用GitHub Pages建立博客 与GitHub建立好链接之后，就可以方便的使用它提供的Pages服务，GitHub Pages分两种，一种是你的GitHub用户名建立的username.github.io这样的用户\u0026amp;组织页（站），另一种是依附项目的pages。\nUser \u0026amp; Organization Pages 想建立个人博客是用的第一种，形如beiyuu.github.io这样的可访问的站，每个用户名下面只能建立一个，创建之后点击Admin进入项目管理，可以看到是这样的： 而普通的项目是这样的，即使你也是用的othername.github.io： 创建好username.github.io项目之后，提交一个index.html文件，然后push到GitHub的master分支（也就是普通意义上的主干）。第一次页面生效需要一些时间，大概10分钟左右。\n生效之后，访问username.github.io就可以看到你上传的页面了，beiyuu.github.io就是一个例子。\n关于第二种项目pages，简单提一下，他和用户pages使用的后台程序是同一套，只不过它的目的是项目的帮助文档等跟项目绑定的内容，所以需要在项目的gh-pages分支上去提交相应的文件，GitHub会自动帮你生成项目pages。具体的使用帮助可以参考Github Pages的官方文档：\n绑定域名 我们在第一部分就提到了在DNS部分的设置，再来看在GitHub的配置，要想让username.github.io能通过你自己的域名来访问，需要在项目的根目录下新建一个名为CNAME的文件，文件内容形如：\nbeiyuu.com  你也可以绑定在二级域名上：\nblog.beiyuu.com  需要提醒的一点是，如果你使用形如beiyuu.com这样的一级域名的话，需要在DNS处设置A记录到207.97.227.245（这个地址会有变动，这里查看），而不是在DNS处设置为CNAME的形式，否则可能会对其他服务（比如email）造成影响。\n设置成功后，根据DNS的情况，最长可能需要一天才能生效，耐心等待吧。\nJekyll模板系统 GitHub Pages为了提供对HTML内容的支持，选择了Jekyll作为模板系统，Jekyll是一个强大的静态模板系统，作为个人博客使用，基本上可以满足要求，也能保持管理的方便，你可以查看Jekyll官方文档。\n你可以直接fork我的项目，然后改名，就有了你自己的满足Jekyll要求的文档了，当然你也可以按照下面的介绍自己创建。\nJekyll基本结构 Jekyll的核心其实就是一个文本的转换引擎，用你最喜欢的标记语言写文档，可以是Markdown、Textile或者HTML等等，再通过layout将文档拼装起来，根据你设置的URL规则来展现，这些都是通过严格的配置文件来定义，最终的产出就是web页面。\n基本的Jekyll结构如下：\n|-- _config.yml |-- _includes |-- _layouts | |-- default.html | `-- post.html |-- _posts | |-- 2007-10-29-why-every-programmer-should-play-nethack.textile | `-- 2009-04-26-barcamp-boston-4-roundup.textile |-- _site `-- index.html  简单介绍一下他们的作用：\n_config.yml 配置文件，用来定义你想要的效果，设置之后就不用关心了。\n_includes 可以用来存放一些小的可复用的模块，方便通过{ % include file.ext %}（去掉前两个{中或者{与%中的空格，下同）灵活的调用。这条命令会调用_includes/file.ext文件。\n_layouts 这是模板文件存放的位置。模板需要通过YAML front matter来定义，后面会讲到，{ { content }}标记用来将数据插入到这些模板中来。\n_posts 你的动态内容，一般来说就是你的博客正文存放的文件夹。他的命名有严格的规定，必须是2012-02-22-artical-title.MARKUP这样的形式，MARKUP是你所使用标记语言的文件后缀名，根据_config.yml中设定的链接规则，可以根据你的文件名灵活调整，文章的日期和标记语言后缀与文章的标题的独立的。\n_site 这个是Jekyll生成的最终的文档，不用去关心。最好把他放在你的.gitignore文件中忽略它。\n其他文件夹 你可以创建任何的文件夹，在根目录下面也可以创建任何文件，假设你创建了project文件夹，下面有一个github-pages.md的文件，那么你就可以通过yoursite.com/project/github-pages访问的到，如果你是使用一级域名的话。文件后缀可以是.html或者markdown或者textile。这里还有很多的例子：https://github.com/mojombo/jekyll/wiki/Sites\nJekyll的配置 Jekyll的配置写在_config.yml文件中，可配置项有很多，我们不去一一追究了，很多配置虽有用但是一般不需要去关心，官方配置文档有很详细的说明，确实需要了可以去这里查，我们主要说两个比较重要的东西，一个是Permalink，还有就是自定义项。\nPermalink项用来定义你最终的文章链接是什么形式，他有下面几个变量：\n year 文件名中的年份 month 文件名中的月份 day 文件名中的日期 title 文件名中的文章标题 categories 文章的分类，如果文章没有分类，会忽略 i-month 文件名中的除去前缀0的月份 i-day 文件名中的除去前缀0的日期  看看最终的配置效果：\n permalink: pretty /2009/04/29/slap-chop/index.html permalink: /:month-:day-:year/:title.html /04-29-2009/slap-chop.html permalink: /blog/:year/:month/:day/:title /blog/2009/04/29/slap-chop/index.html  我使用的是：\n permalink: /:title /github-pages  自定义项的内容，例如我们定义了title:BeiYuu的博客这样一项，那么你就可以在文章中使用{ { site.title }}来引用这个变量了，非常方便定义些全局变量。\nYAML Front Matter和模板变量 对于使用YAML定义格式的文章，Jekyll会特别对待，他的格式要求比较严格，必须是这样的形式：\n--- layout: post title: Blogging Like a Hacker ---  前后的---不能省略，在这之间，你可以定一些你需要的变量，layout就是调用_layouts下面的某一个模板，他还有一些其他的变量可以使用：\n permalink 你可以对某一篇文章使用通用设置之外的永久链接 published 可以单独设置某一篇文章是否需要发布 category 设置文章的分类 tags 设置文章的tag  上面的title就是自定义的内容，你也可以设置其他的内容，在文章中可以通过{ { page.title }}这样的形式调用。\n模板变量，我们之前也涉及了不少了，还有其他需要的变量，可以参考官方的文档：https://github.com/mojombo/jekyll/wiki/template-data\n使用Disqus管理评论 模板部分到此就算是配置完毕了，但是Jekyll只是个静态页面的发布系统，想做到关爽场倒是很容易，如果想要评论呢？也很简单。\n现在专做评论模块的产品有很多，比如Disqus，还有国产的多说，Disqus对现在各种系统的支持都比较全面，到写博客为止，多说现在仅是WordPress的一个插件，所以我这里暂时也使用不了，多说与国内的社交网络紧密结合，还是有很多亮点的，值得期待一下。我先选择了Disqus。\n注册账号什么的就不提了，Disqus支持很多的博客平台，参见下图： 我们选择最下面的Universal Code就好，然后会看到一个介绍页面，把下面这段代码复制到你的模板里面，可以只复制到显示文章的模板中：\n\u0026lt;div id=\u0026quot;disqus_thread\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot;\u0026gt; /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = 'example'; // required: replace example with your forum shortname 这个地方需要改成你配置的网站名 /* * * DON'T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;noscript\u0026gt;Please enable JavaScript to view the \u0026lt;a href=\u0026quot;http://disqus.com/?ref_noscript\u0026quot;\u0026gt;comments powered by Disqus.\u0026lt;/a\u0026gt;\u0026lt;/noscript\u0026gt; \u0026lt;a href=\u0026quot;http://disqus.com\u0026quot; class=\u0026quot;dsq-brlink\u0026quot;\u0026gt;blog comments powered by \u0026lt;span class=\u0026quot;logo-disqus\u0026quot;\u0026gt;Disqus\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;  配置完之后，你也可以做一些异步加载的处理，提高性能，比如我就在最开始页面打开的时候不显示评论，当你想看评论的时候，点击“显示评论”再加载Disqus的模块。代码很简单，你可以参考我的写法。\n$('#disqus_container .comment').on('click',function(){ $(this).html('加载中...'); var disqus_shortname = 'beiyuu'; var that = this; BYB.includeScript('http://' + disqus_shortname + '.disqus.com/embed.js',function(){$(that).remove()}); //这是一个加载js的函数 });  如果你不喜欢Disqus的样式，你也可以根据他生成的HTML结构，自己改写样式覆盖它的，Disqus现在也提供每个页面的评论数接口，帮助文档在这里可以看到。\n代码高亮插件 如果写技术博客，代码高亮少不了，有两个可选插件DlHightLight代码高亮组件和Google Code Prettify。DLHightLight支持的语言相对较少一些，有js、css、xml和html，Google的高亮插件基本上任何语言都支持，也可以自定义语言，也支持自动识别，也有行号的特别支持。\nGoogle的高亮插件使用也比较方便，只需要在\u0026lt;pre\u0026gt;的标签上加入prettyprint即可。所以我选择了Google Code Prettify。\n搭建本地jekyll环境 这里主要介绍一下在Mac OS X下面的安装过程，其他操作系统可以参考官方的jekyll安装。\n作为生活在水深火热的墙内人民，有必要进行下面一步修改gem的源，方便我们更快的下载所需组建：\nsudo gem sources --remove http://rubygems.org/ sudo gem sources -a http://ruby.taobao.org/  然后用Gem安装jekyll\n$ gem install jekyll  不过一般如果有出错提示，你可能需要这样安装：\n$ sudo gem install jekyll  我到了这一步的时候总是提示错误Failed to build gem native extension，很可能的一个原因是没有安装rvm，rvm的安装可以参考这里，或者敲入下面的命令：\n$ curl -L https://get.rvm.io | bash -s stable --ruby  然后还需要安装Markdown的解释器，这个需要在你的_config.yml里面设置markdown:rdiscount：\n$ gem install jekyll rdiscount  好了，如果一切顺利的话，本地环境就基本搭建完成了，进入之前我们建立的博客目录，运行下面的命令：\n$ jekyll serve --watch  这个时候，你就可以通过localhost:4000来访问了。还有关于jekyll bootstrap的资料，需要自己修改调试的，可以研究一下。\n我在这个过程中还遇到两个诡异的没有解决的问题，一个是我放在根目录下面的blog.md等文件，在GitHub的pages服务上一切正常，可以通过beiyuu.com/blog访问的到，但是在本地环境下，总是not found，很是让人郁闷，看生成的_site目录下面的文件，也是正常的blog.html，但就是找不到，只有当我把URL改为localhost:4000/blog.html的时候，才能访问的到，环境不同真糟糕。\n还有一个是关于category的问题，根据YAML的语法，我们在文章头部可以定义文章所属的类别，也可以定义为category:[blog,rss]这样子的多类别，我在本地试一切正常，但是push到GitHub之后，就无法读取了，真让人着急，没有办法，只能采用别的办法满足我的需求了。这里还有一篇Jekyll 本地调试之若干问题，安装中如果有其他问题，也可以对照参考一下。\n结语 如果你跟着这篇不那么详尽的教程，成功搭建了自己的博客，恭喜你！剩下的就是保持热情的去写自己的文章吧。\n[1]: {{ page.url}} ({{ page.title }})\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"988a7d548c842c2b7fa1bf82af94326e","permalink":"/post/2012-02-22-github-pages/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2012-02-22-github-pages/","section":"post","summary":"Github很好的将代码和社区联系在了一起，于是发生了很多有趣的事情，世界也因为他美好了一点点。Github作为现在最流行的代码仓库，已经得","tags":null,"title":"使用Github Pages建独立博客","type":"post"},{"authors":null,"categories":["github technology"],"content":" 参考了一篇非常好的文章，然后结合自己的实际问题，讲解如何使用github结合hexo建立个人的独立博客。\n1. Hexo介绍 Hexo是基于NodeJs的静态博客框架，简单、轻量，其生成的静态网页可以托管在Github和Heroku上。\n. 超快速度 . 支持MarkDown . 一键部署 . 丰富的插件\n下面以我的博客为例，shenxt.github.io为例，讲解如何部署自己的博客。\n2. 环境准备 2.1 安装node.js 去nodejs官网下载对应系统的安装包，按提示安装。\n检验安装成功，在git shell中输入一下代码：\n$ node -v  2.2 安装hexo $ npm install hexo-cli -g  如果是mac，则需要输入：\n$ sudo npm install hexo-cli -g  3. 利用Hexo搭建一个博客 3.1 创建博客目录shenxt@github.io $ hexo init shenxt.github.io $ cd limedroid.github.io $ npm install  3.2 生成静态页面 $ hexo clean $ hexo g # g is generate  3.3 运行 $ hexo s -p3600# is server  然后可以打开浏览器，输入地址 localhost:3600 即可看到效果。\n4 发一篇文章试试 4.1 穿件一个新的博客 $ hexo new test  此时会在source/posts目录下生成test.md文件，输入一些内容，然后保存。\n然后看一下效果:\n$ hexo clean $ hexo g $ hexo s -p3600# is server  然后可以打开浏览器，输入地址 localhost:3600 即可看到效果。\n5 配置 网站的设置大部分都在_config.yml文件夹中，详细配置可以查看官方文档。\n下面只列出简单常用配置:\n.title -\u0026gt; 网站标题 .subtitle -\u0026gt; 网站副标题 .description -\u0026gt; 网站描述 .author -\u0026gt; 您的名字 .language -\u0026gt; 网站使用的语言\n注意：进行配置时，需要在冒号:后加一个英文空格。\n6 更换主题 在网站配置文件_config.yml中，配置theme。\ntheme: next  next是主题的名字。Hexo有不同的人贡献主题，可以到其官方网站上下载不同主题。看中某一主题之后，直接点击其名字，进入到其github界面，然后复制其网址，使用下面代码，即可下载主题到本地。\ngit clone https://github.com/fi3ework/hexo-theme-archer  然后将博客的配置文件theme修改为archer即可。\n观察效果：\n$ hexo clean $ hexo g $ hexo s -p3600# is server  7 部署到github上 7.1 在github网页版上创建和自己账户名相同的仓库，比如我的账户为shenxt，因此，创建的仓库为shenxt.github.io。 7.2 安装hexo-deployer-git $ npm install hexo-deployer-git --save  7.3 网站配置git 在网上的配置文件_config.yml中配置deploy。\ntype: git repo: https://github.com/shenxt/shenxt.github.io branch: master  7.4 部署 $ hexo d# d is deploy  贴标签，方便搜索 8.1 两个确认 . 首先确认博客的配置文件中有：\ntag_dir: tags  . 然后确认主题的配置文件有：\ntags: tags  8.2 新建tags页面 $ hexo new page tags  此时会在source/下生成tags/index.md文件。\n8.3 修改source/tags/index.md title: tags date: 2015-10-20 06:49:50 type: \u0026quot;tags\u0026quot; comments: false  8.4 在文章中添加tags 在你的文章中添加：\ntags: - Tag1 - Tag2 - Tag3  其文件头部类似于：\ntitle: TagEditText date: 2016-11-19 10:44:25 tags: - Tag1 - Tag2 - Tag3  9 分类，给文章归档 9.1 两个确认 . 确认博客配置文件打开了\ncategory_dir: categories  . 确认主题配置文件打开了\ncategories: /categories  9.2 新建categories文件 hexo new page categories  9.3 修改categories/index.md title: categories date: 2015-10-20 06:49:50 type: \u0026quot;categories\u0026quot; comments: false  9.4 在文章中添加categories 在文章中添加：\ncategories: - cate  其文件头部类似：\ntitle: TagEditText date: 2016-11-19 10:44:25 categories: - cate  10 添加评论功能 这里推荐使用韩国的来必力系统。参考这个博客进行设置。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4cae16ead1969e9cec84fd1cc6ce6b2f","permalink":"/post/2017-09-09-github-blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2017-09-09-github-blog/","section":"post","summary":"参考了一篇非常好的文章，然后结合自己的实际问题，讲解如何使用github结合hexo建立个人的独立博客。 1. Hexo介绍 Hexo是基于Node","tags":["Chinese","Github","Blog"],"title":"使用Github和Hexo建独立博客","type":"post"},{"authors":null,"categories":null,"content":" R语言爬虫 虽然相对于python来说，R语言爬虫并不是那么流行，但是对于比较小的数据爬取量，使用R还是很方便的。R的数据爬取比较流行的是利用XML和RCurl包进行爬取，在这篇博客里面，我就利用XML和RCurl包进行KEGG和HMDB的数据爬取。\n爬取KEGG通路信息 因为我需要的信息是KEGG的通路信息，比较简单，也就是每个通路包含哪些代谢物，只要人的metaboloic pathway，因此，我需要先将KEGG中的通路的网页链接拿到。\nlibrary(XML) library(RCurl) ##从kegg主页上抓取代谢通路的url URL = getURL(\u0026quot;http://www.genome.jp/kegg/pathway.html#global\u0026quot;) doc \u0026lt;- htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath.a \u0026lt;- \u0026quot;//a/@href\u0026quot; node \u0026lt;- getNodeSet(doc, xpath.a) url1 \u0026lt;- sapply(node, as.character) xpath.b \u0026lt;- \u0026quot;//a[@href]\u0026quot; name \u0026lt;- getNodeSet(doc, xpath.b) name \u0026lt;- sapply(name, xmlValue) name2 \u0026lt;- name[59:247] url2 \u0026lt;- url1[59:247] url3 \u0026lt;- url2[grep(\u0026quot;show\u0026quot;, url2)] pathwat.name \u0026lt;- NULL metabolite.id \u0026lt;- list() metabolite.name \u0026lt;- list() for (i in 1:length(url3)) { cat(paste(i,\u0026quot;/\u0026quot;,length(url3))) cat(\u0026quot;\\n\u0026quot;) URL \u0026lt;- paste(\u0026quot;http://www.genome.jp\u0026quot;, url3[i], sep = \u0026quot;\u0026quot;) URL = getURL(URL) doc\u0026lt;-htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath \u0026lt;- \u0026quot;//option[@value='hsa']\u0026quot; node\u0026lt;-getNodeSet(doc, xpath) if (length(node) ==0 ) { cat(\u0026quot;No human pathwat.\u0026quot;) next() }else{ URL \u0026lt;- paste(\u0026quot;http://www.genome.jp\u0026quot;, url3[i], sep = \u0026quot;\u0026quot;) URL \u0026lt;- gsub(pattern = \u0026quot;map=map\u0026quot;, replacement = \u0026quot;map=hsa\u0026quot;, x = URL) doc\u0026lt;-htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath1 \u0026lt;- \u0026quot;//title\u0026quot; node\u0026lt;-getNodeSet(doc, xpath1) pathway.name[i] \u0026lt;- xmlValue(node[[1]]) pathway.name[i] \u0026lt;- substr(pathway.name[i], start = 2, stop = nchar(pathway.name[i])-1) xpath2 \u0026lt;- \u0026quot;//area[@shape='circle']/@title\u0026quot; node\u0026lt;-getNodeSet(doc, xpath2) metabolite \u0026lt;- lapply(node, function(x) as.character(x)) metabolite.name[[i]] \u0026lt;- substr(metabolite, start = 9, nchar(metabolite)-1) metabolite.id[[i]] \u0026lt;- substr(metabolite, start = 1, stop = 6) } }  下面对爬取到的代谢通路进行筛选。\nidx \u0026lt;- which(!is.na(pathway.name)) pathway.name1 \u0026lt;- pathway.name[idx] metabolite.id1 \u0026lt;- metabolite.id[idx] metabolite.name1 \u0026lt;- metabolite.name[idx] pathway.name2 \u0026lt;- pathway.name1[-c(83,84)] metabolite.id2 \u0026lt;- metabolite.id1[-c(83,84)] metabolite.name2 \u0026lt;- metabolite.name1[-c(83,84)]  将爬取到的信息保存输出。\nmet.name \u0026lt;- NULL met.id \u0026lt;- NULL path.name \u0026lt;- NULL for(i in 1:length(pathway.name2)) { met.name[i] \u0026lt;- paste(metabolite.name2[[i]], collapse = \u0026quot;;\u0026quot;) met.id[i] \u0026lt;- paste(metabolite.id2[[i]], collapse = \u0026quot;;\u0026quot;) path.name[i] \u0026lt;- gsub(pattern = \u0026quot;KEGG PATHWAY: \u0026quot;, \u0026quot;\u0026quot;, pathway.name2[i]) path.name[i] \u0026lt;- substr(path.name[i], start = 1, stop = nchar(path.name[i])-23) } kegg \u0026lt;- data.frame(path.name, met.name, met.id) write.csv(kegg, \u0026quot;kegg.csv\u0026quot;, row.names = F) save(path.name, file = \u0026quot;path.name\u0026quot;) save(met.name, file = \u0026quot;met.name\u0026quot;) save(met.id, file = \u0026quot;met.id\u0026quot;) kegg.met \u0026lt;- list() kegg.met[[2]] \u0026lt;- sapply(path.name, list) kegg.met[[1]] \u0026lt;- metabolite.name2 kegg.met[[3]] \u0026lt;- metabolite.id2 names(kegg.met) \u0026lt;- c(\u0026quot;gs\u0026quot;, \u0026quot;pathwaynames\u0026quot;, \u0026quot;metid\u0026quot;) save(kegg.met, file = \u0026quot;kegg.met\u0026quot;)  爬取HMDB通路信息 首先爬取HMDB的通路信息。\n##抓取HMDB通路信息 library(XML) library(RCurl) hmdb.main \u0026lt;- \u0026quot;http://www.hmdb.ca/pathways?page=\u0026quot; hmdb.main \u0026lt;- paste(hmdb.main, c(2:46), sep = \u0026quot;\u0026quot;) hmdb.main \u0026lt;- c(\u0026quot;http://www.hmdb.ca/pathways\u0026quot;, hmdb.main) ##从HMDB主页上抓取代谢通路的url path.name \u0026lt;- list() metabolite.id \u0026lt;- list() spec \u0026lt;- list() path.class \u0026lt;- list() for (i in 40:length(hmdb.main)) { cat(paste(\u0026quot;page\u0026quot;,i)) cat(\u0026quot;:\u0026quot;) URL = getURL(hmdb.main[i]) doc\u0026lt;-htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath1 \u0026lt;- \u0026quot;//div[@class='panel-heading']\u0026quot; node1 \u0026lt;- getNodeSet(doc, xpath1) pathway.name \u0026lt;- sapply(node1, xmlValue) cat(paste(length(pathway.name), \u0026quot;pathways\u0026quot;)) cat(\u0026quot;\\n\u0026quot;) path.name[[i]] \u0026lt;- pathway.name xpath2 \u0026lt;- \u0026quot;//div[@class='panel-body']\u0026quot; node2 \u0026lt;- getNodeSet(doc, xpath2) metabolite \u0026lt;- sapply(node2, xmlValue) metabolite \u0026lt;- unname(sapply(metabolite, function(x) {gsub(\u0026quot;Show\u0026quot;, \u0026quot; \u0026quot;, x)})) idx \u0026lt;- sapply(metabolite, function(x) {gregexpr(\u0026quot;HMDB[0-9]{5}\u0026quot;, x)}) met.id \u0026lt;- list() for (j in 1:length(idx)) { id \u0026lt;- NULL for (k in 1:length(idx[[j]])) { id[k] \u0026lt;- substr(metabolite[j], idx[[j]][k], idx[[j]][k]+8) } met.id[[j]] \u0026lt;- id } metabolite.id[[i]] \u0026lt;- met.id xpath.a \u0026lt;- \u0026quot;//a[@class='link-out']/@href\u0026quot; node\u0026lt;-getNodeSet(doc, xpath.a) url1 \u0026lt;- sapply(node, as.character) url1 \u0026lt;- substr(url1, start = 1, stop = 29) url1 \u0026lt;- url1[!duplicated(url1)] ###获取通路的人种和类别 species \u0026lt;- NULL metabolic \u0026lt;- NULL for (t in 1:length(url1)) { cat(paste(\u0026quot;t:\u0026quot;,t));cat(\u0026quot; \u0026quot;) URL = getURL(url1[t]) doc \u0026lt;- htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath \u0026lt;- \u0026quot;//div[@class='species']/text()\u0026quot; node \u0026lt;- getNodeSet(doc, xpath) species[t] \u0026lt;- xmlValue(node[[1]]) xpath \u0026lt;- \u0026quot;//div[@id='des_subject']/text()\u0026quot; node \u0026lt;- getNodeSet(doc, xpath) metabolic[t] \u0026lt;- xmlValue(node[[1]]) } spec[[i]] \u0026lt;- species path.class[[i]] \u0026lt;- metabolic }  对爬取到的代谢通路进行筛选。\nsave(path.name, file = \u0026quot;path.name\u0026quot;) save(metabolite.id, file = \u0026quot;metabolite.id\u0026quot;) save(spec, file = \u0026quot;spec\u0026quot;) save(path.class, file = \u0026quot;path.class\u0026quot;) pathway.name \u0026lt;- NULL metabolite.ID \u0026lt;- list() species \u0026lt;- NULL pathway.class \u0026lt;- NULL for (i in 1:length(path.name)) { pathway.name \u0026lt;- c(pathway.name, path.name[[i]]) metabolite.ID \u0026lt;- c(metabolite.ID, metabolite.id[[i]]) species \u0026lt;- c(species, spec[[i]]) pathway.class \u0026lt;- c(pathway.class, path.class[[i]]) } pathway.class \u0026lt;- substr(x = pathway.class, 1, regexpr(\u0026quot;\\\\\\n\u0026quot;, pathway.class)-1) metabolite.name \u0026lt;- list() for (i in 1:length(metabolite.ID)) { id \u0026lt;- metabolite.ID[[i]] idx \u0026lt;- match(id, hmdbdatabase[,1]) name \u0026lt;- hmdbdatabase[idx,2] metabolite.name[[i]] \u0026lt;- name } a \u0026lt;- unlist(lapply(metabolite.name, function(x) {paste(x, collapse = \u0026quot;;\u0026quot;)})) b \u0026lt;- unlist(lapply(metabolite.ID, function(x) {paste(x, collapse = \u0026quot;;\u0026quot;)})) idx \u0026lt;- grep(\u0026quot;Metabolic\u0026quot;, pathway.class) metabolite.name \u0026lt;- metabolite.name[idx] metabolite.ID \u0026lt;- metabolite.ID[idx] pathway.name \u0026lt;- pathway.name[idx] pathway.class \u0026lt;- pathway.class[idx] species \u0026lt;- species[idx] hmdb.pathway \u0026lt;- data.frame(pathway.name, pathway.class,a, b)[idx,] write.csv(hmdb.pathway, \u0026quot;hmdb.pathway.csv\u0026quot;) a \u0026lt;- list() for (i in 1:length(pathway.name)) { a[[i]] \u0026lt;- pathway.name[i] } pathway.name \u0026lt;- a hmdb.met \u0026lt;- list(gs = metabolite.name, pathwaynames = pathway.name, id = metabolite.ID) save(hmdb.met, file = \u0026quot;hmdb.met\u0026quot;)  爬取HMDB代谢物信息 首先，获得所有代谢物的页面链接。\n###抓取HMDB代谢物信息 library(XML) library(RCurl) hmdb.main \u0026lt;- \u0026quot;http://www.hmdb.ca/metabolites?c=hmdb_id\u0026amp;d=up\u0026amp;page=\u0026quot; hmdb.main \u0026lt;- paste(hmdb.main, c(2:1681), sep = \u0026quot;\u0026quot;) hmdb.main \u0026lt;- c(\u0026quot;http://www.hmdb.ca/metabolites\u0026quot;, hmdb.main) ##从HMDB主页上抓取代谢物的url url \u0026lt;- NULL for (i in 1:length(hmdb.main)) { cat(i) cat(\u0026quot; \u0026quot;) URL = getURL(hmdb.main[i]) doc\u0026lt;-htmlParse(URL,encoding=\u0026quot;utf-8\u0026quot;) xpath \u0026lt;- \u0026quot;//a[@href]/@href\u0026quot; node\u0026lt;-getNodeSet(doc, xpath) url1 \u0026lt;- sapply(node, as.character) url1 \u0026lt;- url1[grep(\u0026quot;metabolites/HMDB\u0026quot;, url1)] url1 \u0026lt;- unique(url1) url \u0026lt;- c(url, url1) } url1 \u0026lt;- paste(\u0026quot;http://www.hmdb.ca/\u0026quot;,url, sep = \u0026quot;\u0026quot;) save(url1, file = \u0026quot;url1\u0026quot;)  下面开始进行代谢物信息爬取。\nlibrary(mailR) for (i in 1:400) { cat(paste((i-1)*100+1,\u0026quot;-\u0026quot;,i*100,\u0026quot;/\u0026quot;, length(url1), sep = \u0026quot;\u0026quot;)) cat(\u0026quot;\\n\u0026quot;) URL \u0026lt;- getURL(url1[((i-1)*100+1):(i*100)]) doc \u0026lt;- htmlParse(URL, encoding=\u0026quot;utf-8\u0026quot;) xpath1 \u0026lt;- \u0026quot;//tr\u0026quot; node1 \u0026lt;- getNodeSet(doc, xpath1) node1 \u0026lt;- sapply(node1, xmlValue) HMDB_ID[((i-1)*100+1):(i*100)] \u0026lt;- gsub(pattern = \u0026quot;HMDB ID\u0026quot;, replacement = \u0026quot;\u0026quot;,node1[grep(\u0026quot;HMDB ID\u0026quot;, node1)]) Common_Name[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;Common Name\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Common Name\u0026quot;, node1)]) temp \u0026lt;- gsub(\u0026quot;SynonymsValueSource\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Synonyms\u0026quot;, node1)]) temp \u0026lt;- gsub(\u0026quot;Generator\u0026quot;, \u0026quot;;\u0026quot;,temp) temp \u0026lt;- gsub(\u0026quot;ChEMBL\u0026quot;, \u0026quot;;\u0026quot;,temp) temp \u0026lt;- gsub(\u0026quot;ChEBI\u0026quot;, \u0026quot;;\u0026quot;,temp) Synonyms[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;HMDB\u0026quot;, \u0026quot;;\u0026quot;,temp) Chemical_Formula[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;Chemical Formula\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Chemical Formula\u0026quot;, node1)]) Monoisotopic_Molecular_Weight[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;Monoisotopic Molecular Weight\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Monoisotopic Molecular Weight\u0026quot;, node1)]) IUPAC_Name[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;IUPAC Name\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;IUPAC Name\u0026quot;, node1)]) Traditional_Name[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;Traditional Name\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Traditional Name\u0026quot;, node1)]) CAS_Registry_Number[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;CAS Registry Number\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;CAS Registry Number\u0026quot;, node1)]) Origin[((i-1)*100+1):(i*100)] \u0026lt;- gsub(\u0026quot;Origin\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Origin\u0026quot;, node1)]) path \u0026lt;- gsub(\u0026quot;PathwaysNameSMPDB LinkKEGG Link\u0026quot;, \u0026quot;\u0026quot;,node1[grep(\u0026quot;Pathways\u0026quot;, node1)]) Pathways[((i-1)*100+1):(i*100)] \u0026lt;- substr(path, 1, stop = regexpr(\u0026quot;SMP\u0026quot;, path)-1) ##每100次保存一次 if (i*100 %in% seq(100, 60000, by = 100)) { cat(\u0026quot;save data...\\n\u0026quot;) save(HMDB_ID, Common_Name, Synonyms, Chemical_Formula, Monoisotopic_Molecular_Weight, IUPAC_Name, Traditional_Name, CAS_Registry_Number, Origin, Pathways, file = paste(\u0026quot;hmdb.data\u0026quot;,i*100)) send.mail(from = \u0026quot;yourmail1@163.com\u0026quot;, to = c(\u0026quot;youmail20@163.com\u0026quot;), subject = paste(\u0026quot;WZZ GO ON:\u0026quot;, i), body = paste(\u0026quot;WZZ still go on\u0026quot;, i), smtp = list(host.name = \u0026quot;smtp.163.com\u0026quot;, port = 465, user.name = \u0026quot;yourmail1\u0026quot;, passwd = \u0026quot;passward\u0026quot;, ssl = TRUE), authenticate = TRUE, send = TRUE) } }  因为代谢物信息比较大，可能需要一晚上，因此想到了没爬取100个，就给自己发一封邮件，来对程序进行监控。\n写的比较粗糙，有时间再好好修改一下。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d246c4943b10304877e7756ec4e04a1a","permalink":"/post/2016-12-03-keggandhmdb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2016-12-03-keggandhmdb/","section":"post","summary":"R语言爬虫 虽然相对于python来说，R语言爬虫并不是那么流行，但是对于比较小的数据爬取量，使用R还是很方便的。R的数据爬取比较流行的是利用","tags":null,"title":"使用R爬取HMDB和KEGG数据库","type":"post"},{"authors":null,"categories":null,"content":" 我的博客 我的博客使用markdown编写，使用的编辑器是ATOM，使用起来还是非常方便的。以后有时间，就用博客来记录我的学习，生活和工作。\n申祖涛于上海\n2016年11月25日晚8点十分\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eeb7c92c1dc622936820385542ff4ec4","permalink":"/post/2016-11-25-first-blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2016-11-25-first-blog/","section":"post","summary":"我的博客 我的博客使用markdown编写，使用的编辑器是ATOM，使用起来还是非常方便的。以后有时间，就用博客来记录我的学习，生活和工作。 申","tags":null,"title":"我的第一篇博客","type":"post"}]